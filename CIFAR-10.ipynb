{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "#checking if its using my gpu or cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Splitting the Dataset (Train & Validation Sets)\n",
    "\n",
    "data_dir = \"./Data/train\"\n",
    "csv_path = \"./Data/trainLabels.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "class_names = sorted(df['label'].unique())\n",
    "class_map = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(val_df)}\")  "
   ],
   "id": "76e6520ff7235fcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Checking if the labels are correctly assigned\n",
    "image_id = 1009\n",
    "\n",
    "print(f\"Label for image {image_id}: {df.loc[df['id'] == image_id, 'label'].values[0]}\")"
   ],
   "id": "9914a7e57dd2604f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Image pre-processing\n",
    "\n",
    "class CIFARDataset(Dataset):\n",
    "    def __init__(self, dataframe, data_dir, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True) #reset index incase if its shuffled\n",
    "        self.data_dir = data_dir #path to where images are stores\n",
    "        self.transform = transform #transformations to preprocess imgs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) #returns total number of imgs from the dataset\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx] #get a row from the dataframe\n",
    "        image_id = row['id'] #get image file name\n",
    "        label_str = row['label'] #get label of the image\n",
    "        \n",
    "        img_path = os.path.join(self.data_dir, f\"{image_id}.png\") #construct path to img\n",
    "        image = Image.open(img_path).convert(\"RGB\") #open image and ensure its rgb\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image) #apply transformations to image\n",
    "        \n",
    "        label = class_map[label_str] #convert label to its numerical label\n",
    "        return image, label #returning the image tensor and its label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), #resize image to 32x32 incase its not\n",
    "    transforms.ToTensor(), #convert the image to a pytorch tensor\n",
    "])\n",
    "\n",
    "train_dataset = CIFARDataset(train_df, data_dir, transform=transform)\n",
    "val_dataset = CIFARDataset(val_df, data_dir, transform=transform)\n",
    "\n",
    "batch_size = 128  # tunable\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #shuffle helps to generalize the data its training on, also to reduce overfitting\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) #data is in order no need to shuffle since its a validation set\n",
    "\n",
    "print(\"Ready!\") #just to lmk if it didnt break\n"
   ],
   "id": "ac08b47e5d2b01ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#CNN model architecture\n",
    "#Brownies: Got batch normalization here\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        #first convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) #3 channels, 32 filters, 3x3 kernels, padding is 1 to prevent image size reduction\n",
    "        self.bn1 = nn.BatchNorm2d(32) #to normalize the activation, stabilize training.\n",
    "        self.pool = nn.MaxPool2d(2, 2) #halves the size of the feature maps. Helps with perfomance (also it retains important info so its fine)\n",
    "        \n",
    "        #second convolution layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        #third convolution layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        #final layer\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256) #flattens the 2048(128*4*4) inputs to 256 neurons.\n",
    "        self.dropout = nn.Dropout(0.3) #drops 30% of the neurons at random, also to prevent overfitting.\n",
    "        self.fc2 = nn.Linear(256, num_classes) #another flatten layer to bring the 256 inputs down to 10 (number of classes of ciphar-10 to correspond to each category of image)\n",
    "        \n",
    "#flow of the data\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) \n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(class_names) #number of unique labels \n",
    "model = CNNModel(num_classes).to(device)\n",
    "print(model) #displays the architecture\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\") #Jst for me to see how many params the model consists of"
   ],
   "id": "be84c75a4bcc47d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) #adam optimizer, super popular. Adapts learning rates for each parameter to improve convergence\n",
    "criterion = nn.CrossEntropyLoss() #good for categorical data\n",
    "\n",
    "#early stopping (if you wanna tweak it)\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "#Do how much ever you want, im keeping it to just 15. Model starts overfitting around 11 epochs.\n",
    "num_epochs = 15\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []"
   ],
   "id": "c8393ff6e8ae2994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#tensorboard cuz they have a nice UI (also I can compare the VGG model and this one over there)\n",
    "\n",
    "writer = SummaryWriter(log_dir='Logs/no_brownies')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    #loading training data per batch\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad() #clearing gradients\n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) #computing loss\n",
    "        loss.backward() #compute the gradients (backpropagation)\n",
    "        optimizer.step() #updates weights\n",
    "        \n",
    "        #tracking loss and accuracy calculations\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    #tracking loss and accuracy calculations\n",
    "    \n",
    "    train_epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct, val_total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            \n",
    "            val_running_loss += val_loss.item() * val_images.size(0)\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "            val_total += val_labels.size(0)\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_epoch_acc = 100.0 * val_correct / val_total\n",
    "    \n",
    "    train_losses.append(train_epoch_loss)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    train_accs.append(train_epoch_acc)\n",
    "    val_accs.append(val_epoch_acc)\n",
    "    \n",
    "    #adding all the data to tensorboard log files.\n",
    "    \n",
    "    writer.add_scalar('Loss/Train', train_epoch_loss, epoch)\n",
    "    writer.add_scalar('Loss/Val', val_epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', train_epoch_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/Val', val_epoch_acc, epoch)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_acc:.2f}% \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%\")\n",
    "    \n",
    "    #early stopping algorithm\n",
    "    \n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "writer.close()"
   ],
   "id": "cfabf410c4170bde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Test accuracy for the best model we generated\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#all the data asked for in the question.\n",
    "\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Validation Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1-score: {f1:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ],
   "id": "b49b4d93144d591f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "639f30f128aa5b8b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
